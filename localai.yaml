name: phi-2
context_size: 2048
f16: true
threads: 11
gpu_layers: 90
mmap: true
parameters:
  model: huggingface:TheBloke/phi-2-GGUF
  temperature: 0.2
  top_k: 40
  top_p: 0.95
  template:
    chat: &template |  
      Instruct: {{.Input}}
      Output: # Modifiez le modèle de prompt ici selon vos besoins
    completion: *template
name: guillaumetell-7b
parameters:
  model: huggingface:AgentPublic/guillaumetell-7b
  temperature: 0.2
  top_k: 40
  top_p: 0.95
template:
  chat: &template |  
    Instruct: {{.Input}}
    Output: # Modifiez le modèle de prompt ici selon vos besoins
  completion: *template
name: french-alpaca-llama3-8B-Q4-GGUF
parameters:
  model: huggingface:jpacifico/french-alpaca-llama3-8B-Q4-GGUF
  temperature: 0.2
  top_k: 40
  top_p: 0.95
template:
  chat: &template |  
    Instruct: {{.Input}}
    Output: # Modifiez le modèle de prompt ici selon vos besoins
  completion: *template
